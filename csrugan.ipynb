{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtNFyYTNQFRK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,Dataset,random_split\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f0vs2lhU7yF"
      },
      "outputs": [],
      "source": [
        "image_dir=r\"/content/drive/MyDrive/CVC/converted\"\n",
        "mask_dir=r\"/content/drive/MyDrive/CVC/converted_labels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld-Bfw78VHFB"
      },
      "outputs": [],
      "source": [
        "sorted_image_files=sorted(os.listdir(image_dir))\n",
        "sorted_mask_files=sorted(os.listdir(mask_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH5911zAVHlT"
      },
      "outputs": [],
      "source": [
        "image_paths=[]\n",
        "mask_paths=[]\n",
        "image_paths = [os.path.join(image_dir, img) for img in sorted_image_files]\n",
        "mask_paths = [os.path.join(mask_dir, msk) for msk in sorted_mask_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAShqU8LVJmc"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(image_paths[9])\n",
        "if image is None:\n",
        "    raise ValueError(f\"Image at {image_paths[9]} could not be loaded.\")\n",
        "\n",
        "mask = cv2.imread(mask_paths[9], cv2.IMREAD_GRAYSCALE)\n",
        "if mask is None:\n",
        "    raise ValueError(f\"Mask at {mask_paths[9]} could not be loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uKeetCGVMKT"
      },
      "outputs": [],
      "source": [
        "class data(Dataset):\n",
        "  def __init__(self,image_paths,mask_paths,transform=None):\n",
        "    self.img_paths=image_paths\n",
        "    self.label_paths=mask_paths\n",
        "    self.transform=transform\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "  def __getitem__(self,idx):\n",
        "    image=cv2.imread(self.img_paths[idx])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mask=cv2.imread(self.label_paths[idx],cv2.IMREAD_GRAYSCALE)\n",
        "    if self.transform is not None:\n",
        "      transformed_data=self.transform(image=image,mask=mask)\n",
        "      image, mask = transformed_data[\"image\"], transformed_data[\"mask\"]\n",
        "    return image,mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MxTOid_Q1Sw"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
        "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "    A.MotionBlur(blur_limit=5, p=0.2),\n",
        "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "dataset=data(image_paths,mask_paths,transform)\n",
        "train_size=int(len(dataset)*(0.75))\n",
        "valid_size=len(dataset)-train_size\n",
        "train_dataset,val_dataset=random_split(dataset,[train_size,valid_size])\n",
        "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
        "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjHpsXFmVxhA"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPBqVUSIWPFx"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYgAhwQGXHRj"
      },
      "outputs": [],
      "source": [
        "from transformers import SwinModel\n",
        "model=\"microsoft/swin-tiny-patch4-window7-224\"\n",
        "swin=SwinModel.from_pretrained(model)\n",
        "print(\"Architecture:\")\n",
        "print(swin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma288vSgpsRN"
      },
      "outputs": [],
      "source": [
        "swin_bottleneck_stage = swin.encoder.layers[2].blocks\n",
        "for param in swin_bottleneck_stage.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdQAf5Tjfmj2"
      },
      "outputs": [],
      "source": [
        "class BottleNeck(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_out_channels, swin_blocks_module_list, resolution=(14, 14)):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.swin_blocks = swin_blocks_module_list\n",
        "\n",
        "\n",
        "        swin_channels = 384\n",
        "\n",
        "        # This is a key component: a 1x1 convolution layer to adapt\n",
        "\n",
        "        self.swin_channel_adapter = nn.Conv2d(\n",
        "            in_channels=encoder_out_channels,\n",
        "            out_channels=swin_channels,\n",
        "            kernel_size=1\n",
        "        )\n",
        "        self.re_adapter=nn.Conv2d(in_channels=swin_channels,\n",
        "                                  out_channels=1024,\n",
        "                                  kernel_size=1)\n",
        "        self.resolution = resolution\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the bottleneck.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input feature map from the encoder,\n",
        "                             with shape (B, encoder_out_channels, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The processed feature map for the decoder,\n",
        "                          with shape (B, swin_channels, H, W).\n",
        "        \"\"\"\n",
        "        # 1. Adapt the channels from the encoder (e.g., 512) to the Swin blocks (384).\n",
        "        # Input shape: (B, 512, 14, 14) -> Output shape: (B, 384, 14, 14)\n",
        "        x = self.swin_channel_adapter(x)\n",
        "\n",
        "        # 2. Permute the tensor for the Swin blocks.\n",
        "        # The Swin blocks expect a shape of (B, H, W, C).\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "        height,width=self.resolution\n",
        "        # 3. Pass through each Swin block in the ModuleList.\n",
        "        # This is where the core feature processing happens.\n",
        "        x=x.view(x.shape[0],height*width,x.shape[-1])\n",
        "\n",
        "        for block in self.swin_blocks:\n",
        "            # Each SwinBlock needs the hidden_states and input_resolution.\n",
        "            x = block(x, self.resolution)[0]\n",
        "\n",
        "        # 4. Permute the tensor back to the standard (B, C, H, W) format for the decoder.\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        batch_size=x.shape[0]\n",
        "        channels=x.shape[1]\n",
        "        x=x.view(batch_size,channels,height,width)\n",
        "        x=self.re_adapter(x)\n",
        "\n",
        "        # The output of this bottleneck is a 14x14 feature map with 384 channels.\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q69JuQ_fqwqu"
      },
      "outputs": [],
      "source": [
        "test_input=torch.randn(1,512,14,14)\n",
        "print(test_input)\n",
        "model=BottleNeck(512,swin_bottleneck_stage,resolution=(14,14))\n",
        "output=model(test_input)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiLSTi1bWV_i"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\n",
        "        super(Generator, self).__init__()\n",
        "        self.features = features\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.channel_adjust = nn.ModuleList()\n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        for feature in self.features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            if in_channels != feature:\n",
        "                self.channel_adjust.append(nn.Conv2d(in_channels, feature, kernel_size=1))\n",
        "            else:\n",
        "                self.channel_adjust.append(nn.Identity())\n",
        "            in_channels = feature\n",
        "\n",
        "        # Replace bottleneck with Swin Transformer\n",
        "        self.BottleNeck = BottleNeck(512,swin_bottleneck_stage,resolution=(14,14))\n",
        "\n",
        "        for feature in reversed(self.features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        self.final_conv = nn.Conv2d(self.features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        skip_connections = []\n",
        "        for idx, down in enumerate(self.downs):\n",
        "\n",
        "            identity = x\n",
        "            x = down(x)\n",
        "            identity = self.channel_adjust[idx](identity)\n",
        "            if x.shape[2:] != identity.shape[2:]:\n",
        "                identity = TF.resize(identity, size=x.shape[2:])\n",
        "            x = x + identity\n",
        "            skip_connections.append(x)\n",
        "            x = self.MaxPool(x)\n",
        "\n",
        "\n",
        "        x = self.BottleNeck(x)  # Swin Transformer bottleneck\n",
        "\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "\n",
        "\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](concat_skip)\n",
        "\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4FXCBklHWDS"
      },
      "outputs": [],
      "source": [
        "test_input=torch.randn(1,3,224,224)\n",
        "model=Generator(in_channels=3,out_channels=1,features=[64,128,256,512])\n",
        "output=model(test_input)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc4FITgVWfp4"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=4, features=[64, 128, 256, 512]):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Build the convolutional layers of the discriminator\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            # The first layer takes the concatenated input\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Build the rest of the layers\n",
        "        for i in range(1, len(features)):\n",
        "            in_feat = features[i - 1]\n",
        "            out_feat = features[i]\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_feat, out_feat, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                    nn.BatchNorm2d(out_feat),\n",
        "                    nn.LeakyReLU(0.2, inplace=True)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        # The final convolutional layer to produce a single output\n",
        "        # (a probability score)\n",
        "        self.final_conv = nn.Conv2d(features[-1], 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        ".\n",
        "\n",
        "        # Pass the concatenated tensor through the main model\n",
        "        x = self.model(x)\n",
        "\n",
        "        # Apply the final convolutional layer to get a single output score\n",
        "        return self.final_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0kkCSna2exx"
      },
      "outputs": [],
      "source": [
        "test_input=torch.randn(1,4,224,224)\n",
        "model=Discriminator(in_channels=4,features=[64,128,256,512])\n",
        "output=model(test_input)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhXX2-XdWmNz"
      },
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.constant_(m.weight, 1.0)\n",
        "        nn.init.constant_(m.bias, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGrwKr_eW4B9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss and Metrics (as provided)\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "        BCE = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
        "        loss = self.alpha * (1 - pt) ** self.gamma * BCE\n",
        "        return loss.mean()\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weight_dice=0.7, weight_bce=0.2, weight_focal=0.7):\n",
        "        super(ComboLoss, self).__init__()\n",
        "        self.weight_dice = weight_dice\n",
        "        self.weight_bce = weight_bce\n",
        "        self.weight_focal = weight_focal\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.focal = FocalLoss(alpha=0.25, gamma=2)\n",
        "\n",
        "    def forward(self, preds, masks):\n",
        "        smooth = 1e-6\n",
        "        preds_sigmoid = torch.sigmoid(preds)\n",
        "        intersection = (preds_sigmoid * masks).sum()\n",
        "        dice_loss = 1 - (2.0 * intersection + smooth) / (preds_sigmoid.sum() + masks.sum() + smooth)\n",
        "        bce_loss = self.bce(preds, masks)\n",
        "        focal_loss = self.focal(preds, masks)\n",
        "        return (self.weight_dice * dice_loss +\n",
        "                self.weight_bce * bce_loss +\n",
        "                self.weight_focal * focal_loss)\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    intersection = (y_true * y_pred).sum()\n",
        "    return (2. * intersection + smooth) / (y_true.sum() + y_pred.sum() + smooth)\n",
        "\n",
        "def iou(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    intersection = (y_true * y_pred).sum()\n",
        "    union = y_true.sum() + y_pred.sum() - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return (y_true == y_pred).sum().item() / y_true.size\n",
        "\n",
        "# Training Loop\n",
        "def train(generator, discriminator, dataloader, g_criterion, d_criterion, g_optimizer, d_optimizer, device, update_discriminator=True, lambda_adv=0.1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    generator.BottleNeck.eval()\n",
        "    running_g_loss = 0.0\n",
        "    running_d_loss = 0.0\n",
        "    dice_scores, iou_scores, accuracy_scores = [], [], []\n",
        "\n",
        "    step = 0\n",
        "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
        "        images, masks = images.to(device), masks.to(device) / 255.0\n",
        "        masks = masks.unsqueeze(1)\n",
        "\n",
        "        # Train Generator\n",
        "        g_optimizer.zero_grad()\n",
        "        fake_masks = generator(images)\n",
        "        disc_input_fake=torch.cat([images,fake_masks],dim=1)\n",
        "        disc_pred_fake = discriminator(disc_input_fake)\n",
        "        g_loss = g_criterion(fake_masks, masks) + lambda_adv * d_criterion(disc_pred_fake, torch.ones_like(disc_pred_fake, device=device))\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "        running_g_loss += g_loss.item()\n",
        "\n",
        "        # Compute train metrics\n",
        "        predictions = torch.sigmoid(fake_masks) > 0.3\n",
        "        dice_scores.append(dice_coefficient(masks.cpu().numpy(), predictions.cpu().numpy()))\n",
        "        iou_scores.append(iou(masks.cpu().numpy(), predictions.cpu().numpy()))\n",
        "        accuracy_scores.append(accuracy(masks.cpu().numpy(), predictions.cpu().numpy()))\n",
        "\n",
        "        # Train Discriminator every 3 steps\n",
        "        if update_discriminator and step % 5 == 0:\n",
        "            d_optimizer.zero_grad()\n",
        "            disc_input_real=torch.cat([images,masks],dim=1)\n",
        "            disc_pred_real = discriminator(disc_input_real)\n",
        "            disc_input_fake_detach=torch.cat([images,fake_masks.detach()],dim=1)\n",
        "            disc_pred_fake = discriminator(disc_input_fake_detach)\n",
        "            d_loss = d_criterion(disc_pred_real, torch.ones_like(disc_pred_real, device=device)) + \\\n",
        "                     d_criterion(disc_pred_fake, torch.zeros_like(disc_pred_fake, device=device))\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "            running_d_loss += d_loss.item()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    train_dice = np.mean(dice_scores)\n",
        "    train_iou = np.mean(iou_scores)\n",
        "    train_acc = np.mean(accuracy_scores)\n",
        "    return running_g_loss / len(dataloader), running_d_loss / len(dataloader), train_dice, train_iou, train_acc\n",
        "def validate(generator, dataloader, criterion, device):\n",
        "    generator.eval()\n",
        "    running_loss = 0.0\n",
        "    dice_scores, iou_scores, accuracy_scores = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Validating\"):\n",
        "            images, masks = images.to(device), masks.to(device) / 255.0\n",
        "            masks = masks.unsqueeze(1)\n",
        "            fake_masks = generator(images)\n",
        "            loss = criterion(fake_masks, masks)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            predictions = torch.sigmoid(fake_masks) > 0.3\n",
        "            dice_scores.append(dice_coefficient(masks.cpu().numpy(), predictions.cpu().numpy()))\n",
        "            iou_scores.append(iou(masks.cpu().numpy(), predictions.cpu().numpy()))\n",
        "            accuracy_scores.append(accuracy(masks.cpu().numpy(), predictions.cpu().numpy()))\n",
        "\n",
        "    return running_loss / len(dataloader), np.mean(dice_scores), np.mean(iou_scores), np.mean(accuracy_scores)\n",
        "# Main Function\n",
        "def main():\n",
        "\n",
        "    batch_size = 16\n",
        "    epochs = 150\n",
        "    lr = 3e-4\n",
        "    patience = 20\n",
        "    lambda_adv = 0.1  # Adjust the adversarial loss weight\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    generator = Generator(in_channels=3, out_channels=1).to(device)\n",
        "    discriminator = Discriminator(in_channels=4).to(device)\n",
        "    generator.apply(weights_init_normal)\n",
        "    discriminator.apply(weights_init_normal)\n",
        "\n",
        "    g_criterion = ComboLoss()\n",
        "    d_criterion = nn.BCEWithLogitsLoss()\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(g_optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_counter = 0\n",
        "    g_losses, d_losses = [], []\n",
        "    val_losses, val_dice_scores, val_iou_scores, val_acc_scores = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        update_discriminator = epoch % 2 == 0  # Update discriminator every alternate epoch\n",
        "        g_loss, d_loss, train_dice, train_iou, train_acc = train(\n",
        "            generator, discriminator, train_loader, g_criterion, d_criterion, g_optimizer, d_optimizer, device, update_discriminator, lambda_adv\n",
        "        )\n",
        "        val_loss, val_dice, val_iou, val_acc = validate(generator, val_loader, g_criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Train G Loss: {g_loss:.4f}, Train D Loss: {d_loss:.4f} | Train Dice: {train_dice:.4f}, Train IoU: {train_iou:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val IoU: {val_iou:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        g_losses.append(g_loss)\n",
        "        d_losses.append(d_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_dice_scores.append(val_dice)\n",
        "        val_iou_scores.append(val_iou)\n",
        "        val_acc_scores.append(val_acc)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(generator.state_dict(), \"best_generator.pth\")\n",
        "            torch.save(discriminator.state_dict(), \"best_discriminator.pth\")\n",
        "            print(\"Models saved with improved val loss.\")\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "          # Show sample segmentation results from validation set\n",
        "        val_iter = iter(val_loader)\n",
        "        sample_images, sample_masks = next(val_iter)\n",
        "        sample_images, sample_masks = sample_images.to(device), sample_masks.to(device) / 255.0\n",
        "        sample_masks = sample_masks.unsqueeze(1)\n",
        "\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "          sample_preds = generator(sample_images)\n",
        "          sample_preds = torch.sigmoid(sample_preds) > 0.3\n",
        "        # Convert tensors to numpy arrays for plotting\n",
        "        sample_image_np = sample_images[0].cpu().numpy().transpose(1, 2, 0)\n",
        "        sample_mask_np = sample_masks[0].squeeze().cpu().numpy()\n",
        "        sample_pred_np = sample_preds[0].squeeze().cpu().numpy()\n",
        "\n",
        "        # Plot the results\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(sample_image_np)\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(sample_mask_np, cmap='gray')\n",
        "        plt.title('Ground Truth Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(sample_pred_np, cmap='gray')\n",
        "        plt.title(f'Predicted Mask (Epoch {epoch+1})')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "        # --- End of new plotting code ---\n",
        "\n",
        "    epochs_list = range(1, len(g_losses) + 1)\n",
        "\n",
        "    # Plotting Losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_list, g_losses, label='Generator Loss', color='blue')\n",
        "    plt.plot(epochs_list, d_losses, label='Discriminator Loss', color='orange')\n",
        "    plt.plot(epochs_list, val_losses, label='Validation Loss', color='green')\n",
        "    plt.title('Training and Validation Losses over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"losses_plot.png\")\n",
        "\n",
        "    # Plotting Metrics\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_list, val_dice_scores, label='Validation Dice Score', color='red')\n",
        "    plt.plot(epochs_list, val_iou_scores, label='Validation IoU Score', color='purple')\n",
        "    plt.plot(epochs_list, val_acc_scores, label='Validation Accuracy', color='brown')\n",
        "    plt.title('Validation Metrics over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"metrics_plot.png\")\n",
        "\n",
        "    plt.show()\n",
        "if __name__=='__main__':\n",
        "  main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}